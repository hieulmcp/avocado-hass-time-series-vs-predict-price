STATISTICS_AND_LINEAR_ALGEBRA_PRELIMINARIES = '''
    6.3. Measurement Formula (Page: 59)
        - Mean absolute error (MAE): CÃ³ nghÄ©a lÃ  thÆ°á»›c Ä‘o giá»¯a 2 biáº¿n liÃªn tá»¥c (biáº¿n dá»± Ä‘oÃ¡n vÃ  biáº¿n liÃªn tá»¥c)
        - Mean squared error (MSE): BÃ¬nh phÆ°Æ¡ng cá»§a Mean absolute error
        - Root Mean squared error (RMSE): cÄƒn báº­c 2 MSE
        - Total sum of squares (TTS): tá»• há»£p cÃ¡c giÃ¡ trá»‹ y thá»±c táº¿ vs y trung bÃ¬nh bÃ¬nh phÆ°Æ¡ng
        - Explained Sum of Squares (ESS): Tá»• há»£p cÃ¡c giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vs y trung bÃ¬nh bÃ¬nh phÆ°Æ¡ng
        - Residual sum of squares (RSS): tá»• há»£p y dá»± Ä‘oÃ¡n vs y trung bÃ¬nh bÃ¬nh phÆ°Æ¡ng
        - R2 - Coefficient of determination: r2 = 1 - RSS/TSS= ESS/TSS
    6.4. Confusion matrix (page number: 60)
        - Recall
        - Precision
        - Accuracy
        - ð¹1-score
    6.5 Statistical Tests (page number: 61)
        6.5.1 Correlational Test
            â€¢ Pearson correlation: Tests for the strength of the association between two continuous variables.
            â€¢ Spearman correlation: Tests for the strength of the association between two ordinal variables (does
            not rely on the assumption of normal distributed data).
            â€¢ Chi-square: Tests for the strength of the association between two categorical variables.
        6.5.2 Comparison of Means test
            â€¢ Paired T-test: Tests for difference between two related variables.
            â€¢ Independent T-test: Tests for difference between two independent variables.
            â€¢ ANOVA: Tests the difference between group means after any other variance in the outcome variable
            is accounted for.
        6.5.3 Non-parametric Test
            â€¢ Wilcoxon rank-sum test: Tests for difference between two independent variables - takes into account
            magnitude and direction of difference.
            â€¢ Wilcoxon sign-rank test: Tests for difference between two related variables - takes into account magnitude
            and direction of difference.
            â€¢ Sign test: Tests if two related variables are different â€“ ignores magnitude of change, only takes into
            account direction
        
'''

DATA_EXPLORATION = ''' 
    7.1. Univariate Analysis (page: 63)
        7.1.1 Numerical Variables
            - Describe
                    + The describe function in pandas and spark will give us most of the statistical results, such as min, median, max, quartiles and standard deviation. 
                    With the help of the user defined function, you can get even more statistical results.
            - Skew, Kru
        7.1.2 Categorical Variables
            - Compared with the numerical variables, the categorical variables are much more easier to do the exploration
'''
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy
import warnings
import pandas_profiling as pp # tá»•ng quan ban Ä‘áº§u vá» dá»¯ liá»‡u => CÃ i trÃªn nÃ y
from matplotlib import pyplot as plt

from pyspark.sql.functions import col, skewness, kurtosis


from pyspark.sql import functions as F
from pyspark.sql.functions import rank,sum,col
from pyspark.sql import Window



#A. Univariate Analysis
## 1.1. ThÃ´ng kÃª biáº¿n liÃªn tá»¥c trong numerical variable
'''
    Function to union the basic stats results and deciles
    :param df_in: the input dataframe
    :param columns: the cloumn name list of the numerical variable
    :param deciles: the deciles output
    :return : the numerical describe info. of the input dataframe
'''

def describe_pd(df_in, columns, deciles=False):

    if deciles:
        percentiles = np.array(range(0, 110, 10))
    else:
        percentiles = [25, 50, 75]
        
    percs = np.transpose([np.percentile(df_in.select(x).collect(),
    percentiles) for x in columns])
    percs = pd.DataFrame(percs, columns=columns)
    percs['summary'] = [str(p) + '%' for p in percentiles]
    spark_describe = df_in.describe().toPandas()
    new_df = pd.concat([spark_describe, percs],ignore_index=True)
    new_df = new_df.round(2)
    
    return new_df[['summary'] + columns]

### 1.2. ThÃ´ng kÃª hÃ m lá»‡ch pháº£i lá»‡ch trÃ¡i vÃ   nhÃ³n hay báº¹t
'''
    df: dataframe
    var: biáº¿n
'''
def skew_kur_var(df, var):
    return df.select(skewness(var),kurtosis(var)).show()

### 1.3. Histograms valization
def histograms_valization(df, var):
    x = df[var]
    bins = np.arange(0, 100, 5.0)
    plt.figure(figsize=(10,8))
    # the histogram of the data
    plt.hist(x, bins, alpha=0.8, histtype='bar', color='gold', ec='black',weights=np.zeros_like(x) + 100. / x.size)
    plt.xlabel(var)
    plt.ylabel('percentage')
    plt.xticks(bins)
    plt.show()
    plt.savefig(var+".pdf", bbox_inches='tight')

### 1.4. Frequency table- báº£ng táº§n sá»‘ biÃªn categorical variable
def frequency_table_categorical_variable(df, var_groupby, var_statis):
    window = Window.rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)
    tab = df.select([var_groupby,var_statis]).\
        groupBy(var_groupby).\
        agg(F.count(var_statis).alias('num'),
            F.mean(var_statis).alias('avg'),
            F.min(var_statis).alias('min'),
            F.max(var_statis).alias('max')).\
            withColumn('total',sum(col('num')).over(window)).\
            withColumn('Percent',col('Credit_num')*100/col('total')).\
            drop(col('total'))
    return tab

#B. Multivariate Analysis
##B1. Numerical V.S. Numerical
## 2.1. Correlation matrix

from pyspark.mllib.stat import Statistics
import pandas as pd
def correlation_maxtrix(df, num_cols):
    corr_data = df.select(num_cols)
    col_names = corr_data.columns
    features = corr_data.rdd.map(lambda row: row[0:])
    corr_mat=Statistics.corr(features, method="pearson")
    corr_df = pd.DataFrame(corr_mat)
    corr_df.index, corr_df.columns = col_names, col_names
    return corr_df.to_string()

##B2. Categorical V.S. Categorical
## 2.2. Pearsonâ€™s Chi-squared test
### Warning: pyspark.ml.stat is only available in Spark 2.4.0.
from pyspark.ml.linalg import Vectors
from pyspark.ml.stat import ChiSquareTest
def chi_squared_categorical(df, features, label):
    
    r = ChiSquareTest.test(df, features, label).head()
    print("pValues: " + str(r.pValues))
    print("degreesOfFreedom: " + str(r.degreesOfFreedom))
    print("statistics: " + str(r.statistics))


#B3. Numerical V.S. Categorical



DATA_MANIPULATION_FEATURES = '''
    8.1 Feature Extraction (page: 85)
        - NLTK lÃ  thÆ° viá»‡n há»— trá»£ phÃ¢n loáº¡i, táº¡o tá»« gá»‘c, gáº¯n tháº», phÃ¢n tÃ­ch cÃº phÃ¡p, láº­p luáº­n ngá»¯ nghÄ©a, mÃ£ hÃ³a 
        - CÃ¡c thÆ° viá»‡n há»— trá»£ NLP phá»• biáº¿n
            + Äá»ƒ thá»±c hiá»‡n cÃ´ng viá»‡c liÃªn quan Ä‘áº¿n phÃ¢n loáº¡i/ phÃ¢n cá»¥m vÄƒn báº£n thÃ¬ cáº§n pháº£i tiá»n xá»­ lÃ½ vá»›i cÃ¡c cÃ´ng viá»‡c:
                BÆ°á»›c 1: Tokenizer
                BÆ°á»›c 2: StopwordRemover
                BÆ°á»›c 3: nGram
                BÆ°á»›c 4: TF-IDF
                BÆ°á»›c 5: CountVectorizer
                ...
        - CÃ¡c cÃ´ng viá»‡c nÃªn thá»±c hiá»‡n
            BÆ°á»›c 1: Tiá»n xá»­ lÃ½ dá»¯ liá»‡u thÃ´
                - Chuyá»ƒn text vá» chá»¯ thÆ°á»ng
                - Loáº¡i bá» cÃ¡c ká»¹ tá»± Ä‘áº·c biá»‡t náº¿u cÃ³
                - Thay tháº¿ nhá»¯ng emojicon báº±ng text tÆ°Æ¡ng á»©ng
                - Thay tháº¿ báº±ng teencode báº±ng text tÆ°Æ¡ng á»©ng
                - Thay tháº¿ báº±ng punctuation vÃ  number báº±ng khoáº£ng tráº¯ng
                - Thay tháº¿ báº±ng cÃ¡c tá»« sai chÃ­nh táº£ báº±ng khoáº£ng tráº¯ng
                - Thay tháº¿ 1 loáº¡t khoáº£ng tráº¯ng thÃ nh 1 khoáº£ng tráº¯ng
            BÆ°á»›c 2: Chuáº©n hÃ³a unicode tiáº¿ng viá»‡t
            BÆ°á»›c 3: Tokenizer vÄƒn báº£n tiáº¿ng viá»‡t báº±ng thÆ° viá»‡n underthesea (cÃ³ xá»­ lÃ½ ghÃ©p tá»« "KhÃ´ng")
            BÆ°á»›c 4: XÃ³a cÃ¡c stopword tiáº¿ng viá»‡t
    8.2. Feature transform (page: 95)
    8.3. Feature Selection (page: 114)
        - LASSO (ToÃ¡n tá»­ thu nhá» vÃ  chá»n lá»c tá»‘i thiá»ƒu) lÃ  má»™t phÆ°Æ¡ng phÃ¡p há»“i quy liÃªn quan Ä‘áº¿n viá»‡c xá»­ pháº¡t kÃ­ch thÆ°á»›c tuyá»‡t Ä‘á»‘i cá»§a cÃ¡c há»‡ sá»‘ há»“i quy.
            - Báº±ng cÃ¡ch xá»­ pháº¡t (hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng rÃ ng buá»™c tá»•ng cÃ¡c giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i cá»§a cÃ¡c Æ°á»›c tÃ­nh), 
            báº¡n sáº½ gáº·p pháº£i tÃ¬nh huá»‘ng trong Ä‘Ã³ má»™t sá»‘ Æ°á»›c tÃ­nh tham sá»‘ cÃ³ thá»ƒ chÃ­nh xÃ¡c báº±ng khÃ´ng. HÃ¬nh pháº¡t Ä‘Æ°á»£c Ã¡p dá»¥ng cÃ ng lá»›n, Æ°á»›c tÃ­nh cÃ ng bá»‹ thu háº¹p vá» khÃ´ng.
            - Äiá»u nÃ y thuáº­n tiá»‡n khi chÃºng ta muá»‘n má»™t sá»‘ tÃ­nh nÄƒng tá»± Ä‘á»™ng / lá»±a chá»n biáº¿n hoáº·c khi xá»­ lÃ½ cÃ¡c yáº¿u tá»‘ dá»± Ä‘oÃ¡n tÆ°Æ¡ng quan cao, 
            trong Ä‘Ã³ há»“i quy tiÃªu chuáº©n thÆ°á»ng sáº½ cÃ³ cÃ¡c há»‡ sá»‘ há»“i quy 'quÃ¡ lá»›n'.
        - RandomForest
            - https://github.com/runawayhorse001/AutoFeatures
    8.4. Unbalanced data: Undersampling
    
          
'''


from pyspark.ml.feature import Tokenizer, RegexTokenizer
from pyspark.sql.functions import col, udf
from pyspark.sql.types import IntegerType
from pyspark.ml.feature import StopWordsRemover
from pyspark.ml import Pipeline
from pyspark.ml.feature import CountVectorizer
from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from pyspark.ml.feature import NGram
from pyspark.ml.feature import Binarizer
from pyspark.ml.feature import QuantileDiscretizer, Bucketizer
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer,OneHotEncoder, VectorAssembler
from pyspark.sql.functions import col
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, OneHotEncoder,VectorAssembler
from pyspark.sql.functions import col

'''
    parameter:
        - df: thá»ƒ pyspark cá»§a báº£ng dá»¯ liá»‡u gá»“m 2 feature: ID vÃ  sentence
        - inputCol: sentence
        - outputCol: word
        - outputCol_stop_word: removeded
        - outputCol_ngrams: ngrams
        - inputCol_idf = "rawFeatures"
        - outputCol_binarizer="binarized_feature"
    return:
        - 
'''
# 3.1. Thá»±c hiá»‡n cÃ¡c bÆ°á»›c phÃ¢n cá»¥m/ gÃ¡n nhÃ£n feature_transform (page: 95)
def feature_transform(df, threshold=0.5 ,inputCol ="sentence", outputCol="word", outputCol_stop_word = "removeded", outputCol_ngrams="ngrams", inputCol_idf = "rawFeatures", outputCol_idf="features", outputCol_binarizer="binarized_feature"):
    # BÆ°á»›c 1: Tokenizer- phÃ¢n tÃ¡ch tá»«
    tokenizer = Tokenizer(inputCol=inputCol, outputCol=outputCol)
    regexTokenizer = RegexTokenizer(inputCol=inputCol, outputCol=outputCol, pattern="\\W")
    # alternatively, pattern="\\w+", gaps(False)
    countTokens = udf(lambda words: len(words), IntegerType())
    tokenized = tokenizer.transform(df)
    tokenized.select(inputCol, outputCol)\
    .withColumn("tokens", countTokens(col(outputCol))).show(truncate=False)
    regexTokenized = regexTokenizer.transform(df)
    regexTokenized.select(inputCol, outputCol) \
    .withColumn("tokens", countTokens(col(outputCol))).show(truncate=False)
    # BÆ°á»›c 2: Thá»±c hiá»‡n stopword
    remover = StopWordsRemover(inputCol=outputCol, outputCol=outputCol_stop_word)
    #remover.transform(df).show(truncate=False)
    # BÆ°á»›c 3: NGram 
    ngram = NGram(n=2, inputCol=outputCol, outputCol=outputCol_ngrams)
    idf = IDF(inputCol=inputCol_idf, outputCol=outputCol_idf)
    pipeline = Pipeline(stages=[tokenizer, ngram])
    model = pipeline.fit(df)
    model.transform(df).show(truncate=False)
    # BÆ°á»›c 4: Binarizer
    binarizer = Binarizer(threshold=threshold, inputCol=outputCol_idf, outputCol=outputCol_binarizer)
    binarizedDataFrame = binarizer.transform(df)
    return binarizedDataFrame

# 3.2. Bucketizer: PhÃ¢n loáº¡i nhÃ£n cho dá»¯ liá»‡u sá»‘ (page: 98)
def Bucketizer_feature(df, inputCol, outputCol):
    
    splits = [-float("inf"),3, 10,float("inf")]
    result_bucketizer = Bucketizer(splits=splits, inputCol=inputCol,outputCol=outputCol).transform(df)
    
    return result_bucketizer

# Calculate undersampling Ratio

import math
def round_up(n, decimals=0):
    multiplier = 10 ** decimals
    return math.ceil(n * multiplier) / multiplier


Linear_Regression = '''
    page: 128
'''

# Convert the data to dense vector (features and label)

def transData(data):
    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])

# getdummy - Supervised learning version:
def get_dummy_supervised_learning(df, indexCol, categoricalCols, continuousCols, labelCol):
    indexers = [ StringIndexer(inputCol=c, outputCol="{0}_indexed".format(c))for c in categoricalCols ]
    # default setting: dropLast=True
    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),outputCol="{0}_encoded".format(indexer.getOutputCol()))for indexer in indexers ]
    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]+ continuousCols, outputCol="features")
    pipeline = Pipeline(stages=indexers + encoders +[assembler])
    model=pipeline.fit(df)
    data = model.transform(df)
    data = data.withColumn('label',col(labelCol))
    if indexCol:
        return data.select(indexCol,'features','label')
    else:
        return data.select('features','label')
    
# getdummy - Unsupervised learning version:

def get_dummy_unsupervised_learning(df,indexCol,categoricalCols,continuousCols):
    '''
    Get dummy variables and concat with continuous variables for unsupervised learning.
    :param df: the dataframe
    :param categoricalCols: the name list of the categorical data
    :param continuousCols: the name list of the numerical data
    :return k: feature matrix
'''
    indexers = [ StringIndexer(inputCol=c, outputCol="{0}_indexed".format(c)) for c in categoricalCols ]
    # default setting: dropLast=True
    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),
    outputCol="{0}_encoded".format(indexer.getOutputCol())) for indexer in indexers ]
    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]+ continuousCols, outputCol="features")
    pipeline = Pipeline(stages=indexers + encoders +[assembler])
    model=pipeline.fit(df)
    data = model.transform(df)
    if indexCol:
        return data.select(indexCol,'features')
    else:
        return data.select('features')
    

def get_dummy_one_feature_unsupervised_learning(df, indexCol, categoricalCols, continuousCols, labelCol, dropLast=False):
    '''
        Get dummy variables and concat with continuous variables for ml
        Ë“â†’modeling.
        :param df: the dataframe
        :param categoricalCols: the name list of the categorical data
        :param continuousCols: the name list of the numerical data
        :param labelCol: the name of label column
        :param dropLast: the flag of drop last column
        :return: feature matrix
        :author: Wenqiang Feng
        :email: von198@gmail.com
        >>> df = spark.createDataFrame([
        (0, "a"),
        (1, "b"),
        (2, "c"),
        (3, "a"),
        (4, "a"),
        (5, "c")
        ], ["id", "category"])
        >>> indexCol = 'id'
        >>> categoricalCols = ['category']
        >>> continuousCols = []
        >>> labelCol = []
        >>> mat = get_dummy(df,indexCol,categoricalCols,continuousCols,
        Ë“â†’labelCol)
        >>> mat.show()
        >>>
        +---+-------------+
        | id| features|
        +---+-------------+
        | 0|[1.0,0.0,0.0]|
        | 1|[0.0,0.0,1.0]|
        | 2|[0.0,1.0,0.0]|
        | 3|[1.0,0.0,0.0]|
        | 4|[1.0,0.0,0.0]|
        | 5|[0.0,1.0,0.0]|
        +---+-------------+
    '''
    
    indexers = [ StringIndexer(inputCol=c, outputCol="{0}_indexed".format(c))for c in categoricalCols ]
    # default setting: dropLast=True
    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),outputCol="{0}_encoded".format(indexer.getOutputCol()),dropLast=dropLast)for indexer in indexers ]
    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol="features")
    pipeline = Pipeline(stages=indexers + encoders + [assembler])
    model=pipeline.fit(df)
    data = model.transform(df)
    if indexCol and labelCol:
    # for supervised learning
        data = data.withColumn('label',col(labelCol))
        return data.select(indexCol,'features','label')
    elif not indexCol and labelCol:
    # for supervised learning
        data = data.withColumn('label',col(labelCol))
        return data.select('features','label')
    elif indexCol and not labelCol:
    # for unsupervised learning
        return data.select(indexCol,'features')
    elif not indexCol and not labelCol:
    # for unsupervised learning
        return data.select('features')